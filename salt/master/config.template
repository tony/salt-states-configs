##### Primary configuration settings #####
##########################################
# The address of the interface to bind to
#interface: 0.0.0.0
{% if interface is defined -%} interface: {{ interface }}{% endif %}

# The port used by the publisher
#publish_port: 4505
{% if publish_port is defined -%} publish_port: {{ publish_port }}{% endif %}

# The user to run salt
#user: root
{% if salt_user is defined -%} user: {{ salt_user }}{% endif %}

# The number of worker threads to start, these threads are used to manage
# return calls made from minions to the master, if the master seems to be
# running slowly, increase the number of threads
#worker_threads: 5
{% if worker_threads is defined -%} worker_threads: {{ worker_threads }}{% endif %}

# The port used by the communication interface
#ret_port: 4506
{% if ret_port is defined -%} ret_port: {{ ret_port }}{% endif %}

# The root directory prepended to these options: pki_dir, cachedir,
# sock_dir, log_file.
#root_dir: /
{% if root_dir is defined -%} root_dir: {{ root_dir }}{% endif %}

# Directory used to store public key data
#pki_dir: /etc/salt/pki
{% if pki_dir is defined -%} pki_dir: {{ pki_dir }}{% endif %}

# Directory to store job and cache data
#cachedir: /var/cache/salt
{% if cachedir is defined -%} cachedir: {{ cachedir }}{% endif %}

# Set the number of hours to keep old job information
#keep_jobs: 24
{% if keep_jobs is defined -%} keep_jobs: {{ keep_jobs }}{% endif %}

# Set the default timeout for the salt command and api, the default is 5
# seconds
#timeout: 5
{% if timeout is defined -%} timeout: {{ timeout }}{% endif %}

# Set the directory used to hold unix sockets
#sock_dir: /tmp/salt-unix
{% if sock_dir is defined -%} sock_dir: {{ sock_dir }}{% endif %}

# The master maintains a job cache, while this is a great addition it can be
# a burden on the master for larger deployments (over 5000 minions).
# Disabling the job cache will make previously executed jobs unavailable to
# the jobs system and is not generally recommended.
#
#job_cache: True
{% if job_cache is defined -%} job_cache: {{ job_cache }}{% endif %}

# Set the acceptance level for serialization of messages. This should only be
# set if the master is newer than 0.9.5 and the minion are older. This option
# allows a 0.9.5 and newer master to communicate with minions 0.9.4 and
# earlier. It is not recommended to keep this setting on if the minions are
# all 0.9.5 or higher, as leaving pickle as the serialization medium is slow
# and opens up security risks
#
#serial: msgpack
{% if serial is defined -%} serial: {{ serial }}{% endif %}

#####        Security settings       #####
##########################################
# Enable "open mode", this mode still maintains encryption, but turns off
# authentication, this is only intended for highly secure environments or for
# the situation where your keys end up in a bad state. If you run in open mode
# you do so at your own risk!
#open_mode: False
{% if open_mode is defined -%} open_mode: {{ open_mode }}{% endif %}

# Enable auto_accept, this setting will automatically accept all incoming
# public keys from the minions. Note that this is insecure.
#auto_accept: False
{% if auto_accept is defined -%} auto_accept: {{ auto_accept }}{% endif %}

#####    Master Module Management    #####
##########################################
# Manage how master side modules are loaded
#
# Add any additional locations to look for master runners
#runner_dirs: []
{% if runner_dirs is defined -%} runner_dirs: {{ runner_dirs }}{% endif %}
#
#Enable Cython for master side modules
#cython_enable: False
{% if cython_enable is defined -%} cython_enable: {{ cython_enable }}{% endif %}

#####      State System settings     #####
##########################################
# The state system uses a "top" file to tell the minions what environment to
# use and what modules to use. The state_top file is defined relative to the
# root of the base environment as defined in "File Server settings" below.
#state_top: top.sls
{% if state_top is defined -%} state_top: {{ state_top }}{% endif %}
#
# The external_nodes option allows Salt to gather data that would normally be
# placed in a top file. The external_nodes option is the executable that will
# return the ENC data. Remember that Salt will look for external nodes AND top
# files and combine the results if both are enabled!
#external_nodes: None
{% if external_nodes is defined -%} external_nodes: {{ external_nodes }}{% endif %}
#
# The renderer to use on the minions to render the state data
#renderer: yaml_jinja
{% if renderer is defined -%} renderer: {{ renderer }}{% endif %}
#
# The failhard option tells the minions to stop immediately after the first
# failure detected in the state execution, defaults to False
#failhard: False
{% if failhard is defined -%} failhard: {{ failhard }}{% endif %}

# state_verbose allows for the data returned from the minion to be more
# verbose. Normally only states that fail or states that have changes are
# returned, but setting state_verbose to True will return all states that
# were checked
#state_verbose: False
{% if state_verbose is defined -%} state_verbose: {{ state_verbose }}{% endif %}

#####      File Server settings      #####
##########################################
# Salt runs a lightweight file server written in zeromq to deliver files to
# minions. This file server is built into the master daemon and does not
# require a dedicated port.

# The file server works on environments passed to the master, each environment
# can have multiple root directories, the subdirectories in the multiple file
# roots cannot match, otherwise the downloaded files will not be able to be
# reliably ensured. A base environment is required to house the top file.
# Example:
# file_roots:
#   base:
#     - /srv/salt/
#   dev:
#     - /srv/salt/dev/services
#     - /srv/salt/dev/states
#   prod:
#     - /srv/salt/prod/services
#     - /srv/salt/prod/states
#
# Default:
#file_roots:
#  base:
#    - /srv/salt
{% if file_roots is defined -%} file_roots: {{ file_roots }}{% endif %}

# The hash_type is the hash to use when discovering the hash of a file on
# the master server, the default is md5, but sha1, sha224, sha256, sha384
# and sha512 are also supported.
#hash_type: md5
{% if hash_type is defined -%} hash_type: {{ hash_type }}{% endif %}

# The buffer size in the file server can be adjusted here:
#file_buffer_size: 1048576
{% if file_buffer_size is defined -%} file_buffer_size: {{ file_buffer_size }}{% endif %}

# Pillar Configurations:
# The Salt Pillar, is a system that allows for the building of global data
# that is refined based on minion. Basically, the pillar creates data that
# can be generated to be specific based on the grains of the minion. Pillar
# is laid out in the same fashion as the file server, with environments, a top
# file and sls files. The difference is that the data does not need to be
# in the highstate format, and is generally just key/value pairs.
#
#pillar_roots:
#  base:
#    - /srv/pillar
#
#ext_pillar:
#  - hiera: /etc/hiera.yaml
#  - cmd: cat /etc/salt/yaml
#
{% if pillar_roots is defined -%} pillar_roots: {{ pillar_roots }}{% endif %}
{% if ext_pillar is defined -%} ext_pillar: {{ ext_pillar }}{% endif %}

#####          Syndic settings       #####
##########################################
# The Salt syndic is used to pass commands through a master from a higher
# master. Using the syndic is simple, if this is a master that will have
# syndic servers(s) below it set the "order_masters" setting to True, if this
# is a master that will be running a syndic daemon for passthrough the
# "syndic_master" setting needs to be set to the location of the master server
# to receive commands from.
#
# Set the order_masters setting to True if this master will command lower
# masters' syndic interfaces.
#order_masters: False
{% if order_masters is defined -%} order_masters: {{ order_masters }}{% endif %}
#
# If this master will be running a salt syndic daemon, syndic_master tells
# this master where to receive commands from.
#syndic_master: masterofmaster
{% if syndic_master is defined -%} syndic_master: {{ syndic_master }}{% endif %}

#####      Peer Publish settings     #####
##########################################
# Salt minions can send commands to other minions, but only if the minion is
# allowed to. By default "Peer Publication" is disabled, and when enabled it
# is enabled for specific minions and specific commands. This allows secure
# compartmentalization of commands based on individual minions.
#
# The configuration uses regular expressions to match minions and then a list
# of regular expressions to match functions. The following will allow the
# minion authenticated as foo.example.com to execute functions from the test
# and pkg modules.
# peer:
#   foo.example.com:
#       - test.*
#       - pkg.*
#
# This will allow all minions to execute all commands:
# peer:
#   .*:
#       - .*
# This is not recommended, since it would allow anyone who gets root on any
# single minion to instantly have root on all of the minions!
{% if peer is defined -%} peer: {{ peer }}{% endif %}
#
# Minions can also be allowed to execute runners from the salt master.
# Since executing a runner from the minion could be considered a security risk,
# it needs to be enabled. This setting functions just like the peer setting
# except that it opens up runners instead of module functions.
#
# All peer runner support is turned off by default and must be enabled before
# using. This will enable all peer runners for all minions:
#
# peer_run:
#   .*:
#     - .*
#
# To enable just the manage.up runner for the minion foo.example.com:
#
# peer_run:
#   foo.example.com:
#     - manage.up
#
{% if peer_run is defined -%} peer_run: {{ peer_run }}{% endif %}

#####         Cluster settings       #####
##########################################
# Salt supports automatic clustering, salt creates a single ip address which
# is shared among the individual salt components using ucarp. The private key
# and all of the minion keys are maintained across the defined cluster masters.
# The failover service is automatically managed via these settings

# List the identifiers for the other cluster masters in this manner:
# [saltmaster-01.foo.com,saltmaster-02.foo.com,saltmaster-03.foo.com]
# The members of this master array must be running as salt minions to
# facilitate the distribution of cluster information
#cluster_masters: []
{% if cluster_masters is defined -%} cluster_masters: {{ cluster_masters }}{% endif %}

# The cluster modes are "paranoid" and "full"
# paranoid will only distribute the accepted minion public keys.
# full will also distribute the master private key.
#cluster_mode: paranoid
{% if cluster_mode is defined -%} cluster_mode: {{ cluster_mode }}{% endif %}


#####         Logging settings       #####
##########################################
# The location of the master log file
#log_file: /var/log/salt/master
{% if log_file is defined -%} log_file: {{ log_file }}{% endif %}
#
# The level of messages to send to the log file.
# One of 'info', 'quiet', 'critical', 'error', 'debug', 'warning'.
# Default: 'warning'
#log_level: warning
{% if log_level is defined -%} log_level: {{ log_level }}{% endif %}
#
# Logger levels can be used to tweak specific loggers logging levels.
# For example, if you want to have the salt library at the 'warning' level,
# but you still wish to have 'salt.modules' at the 'debug' level:
#   log_granular_levels:
#     'salt': 'warning',
#     'salt.modules': 'debug'
#
#log_granular_levels: {}
{% if log_granular_levels is defined -%} log_granular_levels: {{ log_granular_levels }}{% endif %}


#####         Node Groups           #####
##########################################
# Node groups allow for logical groupings of minion nodes.
# A group consists of a group name and a compound target.
#
# nodegroups:
#   group1: 'L@foo.domain.com,bar.domain.com,baz.domain.com and bl*.domain.com'
#   group2: 'G@os:Debian and foo.domain.com'
{% if nodegroups is defined -%} nodegroups: {{ nodegroups }}{% endif %}

#####     Range Cluster settings     #####
##########################################
# The range server (and optional port) that
# serves your cluster information
#range_server: range:80
{% if range_server is defined -%} range_server: {{ range_server }}{% endif %}
